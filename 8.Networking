###### What is the network interface configured for cluster connectivity on the controlplane node?
```
Run the ip a / ip link command and identify the interface.

root@controlplane:~# ip a | grep 10.0.0.0.0
16476: eth0@if16477: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default 
    link/ether 02:42:0a:03:74:0c brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.3.116.12/24 brd 10.3.116.255 scope global eth0
root@controlplane:~# 
```


###### What is the MAC address of the interface on the controlplane node?
```
ip link show eth0
```

###### What is the MAC address assigned to node01?
```
arp node01
```

###### We use Docker as our container runtime. What is the interface/bridge created by Docker on this host?
```
ip link

docker0
```

###### If you were to ping google from the controlplane node, which route does it take?
```
ip route show default
```

###### What is the port the kube-scheduler is listening on in the controlplane node?
```
netstat -lntp
```

###### Notice that ETCD is listening on two ports. Which of these have more client connections established?
```
netstat -anp | grep etcd

netstat -anp | grep etcd | grep 2380 | wc -l

netstat -anp | grep etcd | grep 2379 | wc -l
```


###### because 2379 is the port of ETCD to which all control plane components connect to. 2380 is only for etcd peer-to-peer connectivity. When you have multiple controlplane nodes. In this case we don't.



### CNI Weave
###### Inspect the kubelet service and identify the network plugin configured for Kubernetes.
```
ps -aux | grep kubelet | grep --color network-plugin=   
```

######  What is the path configured with all binaries of CNI supported plugins?
```
/opt/cni/bin
```

###### Identify which of the below plugins is not available in the list of available CNI plugins on this host?
```
ls /opt/cni/bin
```

###### What is the CNI plugin configured to be used on this kubernetes cluster?
```
ls /etc/cni/net.d/
```

###### What binary executable file will be run by kubelet after a container and its associated namespace are created.
```
vi /etc/cni/net.d/10-flannel.conflist
```

###### Deploy weave-net networking solution to the cluster.Replace the default IP address and subnet of weave-net to the 10.50.0.0/16. Please check the official weave installation and configuration guide which is available at the top right panel.
```
By default, the range of IP addresses and the subnet used by weave-net is 10.32.0.0/12 and it's overlapping with the host system IP addresses.
To know the host system IP address by running ip a command :-

root@controlplane:~# ip a | grep eth0
12396: eth0@if12397: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default 
    inet 10.40.56.3/24 brd 10.40.56.255 scope global eth0
If we deploy a weave manifest file directly without changing the default IP addresses it will overlap with the host system IP addresses and as a result, it's weave pods will go into an Error or CrashLoopBackOff state.

root@controlplane:~# kubectl get po -n kube-system | grep weave
weave-net-6mckb                        1/2     CrashLoopBackOff   6          6m46s
If we will go more deeper and inspect the logs then we can clearly see the issue :-

root@controlplane:~# kubectl logs -n kube-system weave-net-6mckb -c weave
Network 10.32.0.0/12 overlaps with existing route 10.40.56.0/24 on host
So we need to change the default IP address by adding &env.IPALLOC_RANGE=10.50.0.0/16 option at the end of the manifest file. It should be look like as follows :-

kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')&env.IPALLOC_RANGE=10.50.0.0/16"
then run the kubectl get pods -n kube-system to see the status of weave-net pods.
Note :- 10.40.56.3 IP address is used here as an example. It may be different in your assigned lab.

```


